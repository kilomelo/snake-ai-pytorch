{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "# pygame.init()\n",
    "# font = pygame.font.Font('arial.ttf', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# game graphic\n",
    "HEAD = \"@\"\n",
    "WALL = \"#\"\n",
    "BODY = \"O\"\n",
    "FOOD = \"*\"\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self, width, height):\n",
    "        self.map = np.zeros((width, height))\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # init display\n",
    "        # self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        # pygame.display.set_caption('Snake')\n",
    "        # self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.map[:, :] = 0\n",
    "        self.direction = Direction.RIGHT\n",
    "        self.head = Point(self.width // 2, self.height // 2)\n",
    "        self.snake = [\n",
    "            self.head,\n",
    "            Point(self.head.x - 1, self.head.y),\n",
    "            Point(self.head.x - 2, self.head.y)\n",
    "        ]\n",
    "        self.map[self.head.x, self.head.y] = 1\n",
    "        for body in self.snake[1:]:\n",
    "            self.map[body.x, body.y] = 2\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "\n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, self.width - 1) \n",
    "        y = random.randint(0, self.height - 1)\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.map[self.food.x, self.food.y] = 3\n",
    "\n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "\n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "\n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "        \n",
    "        self.map[self.head.x, self.head.y] = 1\n",
    "        \n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            tail = self.snake.pop()\n",
    "            self.map[tail.x, tail.y] = 0\n",
    "\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "    def is_collision(self, pt=None):\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        # hits boundary\n",
    "        if pt.x >= self.width or pt.x < 0 or pt.y >= self.height or pt.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def render(self):\n",
    "        for h in range(self.height + 2):\n",
    "            for w in range(self.width + 2):\n",
    "                p = Point(w - 1, h - 1)\n",
    "                if w == 0 or w == self.width + 1 or h == 0 or h == self.height + 1:\n",
    "                    print(WALL, end = \"\\n\" if w == self.width + 1 else \"\")\n",
    "                elif self.map[p.x, p.y] == 1:\n",
    "                    print(HEAD, end=\"\")\n",
    "                elif self.map[p.x, p.y] == 2:\n",
    "                    print(BODY, end=\"\")\n",
    "                elif self.map[p.x, p.y] == 3:\n",
    "                    print(FOOD, end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "\n",
    "    def _move(self, action):\n",
    "        # [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx] # no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
    "        else: # [0, 0, 1]\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        self.map[x, y] = 2\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += 1\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= 1\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += 1\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= 1\n",
    "\n",
    "        self.head = Point(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot\n",
    "\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 # randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(game.width * game.height, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "    def get_state(self, game):\n",
    "        return game.map.flatten()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "        #for state, action, reward, nexrt_state, done in mini_sample:\n",
    "        #    self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    game = SnakeGame(16, 9)\n",
    "    agent = Agent(game)\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "\n",
    "        # clear_output(wait=True)\n",
    "        # game.render()\n",
    "        # time.sleep(0.01)\n",
    "\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            # mean_score = total_score / agent.n_games\n",
    "            mean_score = average_of_last_n_items(plot_scores, 20)\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            # plot(plot_scores, plot_mean_scores)\n",
    "        \n",
    "\n",
    "def average_of_last_n_items(lst, n):\n",
    "    # 边界情况：当n为0或负数时，返回None\n",
    "    if n <= 0:\n",
    "        return None\n",
    "    \n",
    "    # 边界情况：当列表为空时，返回None\n",
    "    if not lst:\n",
    "        return None\n",
    "\n",
    "    # 如果n大于列表的长度，使用整个列表\n",
    "    n = min(n, len(lst))\n",
    "    \n",
    "    # 使用切片获取末尾n项，并计算平均值\n",
    "    return sum(lst[-n:]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenweichu/dev/study/snake-ai-pytorch/model.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  state = torch.tensor(state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0\n",
      "Game 2 Score 0 Record: 0\n",
      "Game 3 Score 0 Record: 0\n",
      "Game 4 Score 0 Record: 0\n",
      "Game 5 Score 0 Record: 0\n",
      "Game 6 Score 0 Record: 0\n",
      "Game 7 Score 0 Record: 0\n",
      "Game 8 Score 0 Record: 0\n",
      "Game 9 Score 0 Record: 0\n",
      "Game 10 Score 0 Record: 0\n",
      "Game 11 Score 0 Record: 0\n",
      "Game 12 Score 0 Record: 0\n",
      "Game 13 Score 0 Record: 0\n",
      "Game 14 Score 0 Record: 0\n",
      "Game 15 Score 0 Record: 0\n",
      "Game 16 Score 0 Record: 0\n",
      "Game 17 Score 1 Record: 1\n",
      "Game 18 Score 0 Record: 1\n",
      "Game 19 Score 0 Record: 1\n",
      "Game 20 Score 0 Record: 1\n",
      "Game 21 Score 0 Record: 1\n",
      "Game 22 Score 0 Record: 1\n",
      "Game 23 Score 0 Record: 1\n",
      "Game 24 Score 0 Record: 1\n",
      "Game 25 Score 0 Record: 1\n",
      "Game 26 Score 1 Record: 1\n",
      "Game 27 Score 1 Record: 1\n",
      "Game 28 Score 1 Record: 1\n",
      "Game 29 Score 0 Record: 1\n",
      "Game 30 Score 0 Record: 1\n",
      "Game 31 Score 0 Record: 1\n",
      "Game 32 Score 0 Record: 1\n",
      "Game 33 Score 0 Record: 1\n",
      "Game 34 Score 0 Record: 1\n",
      "Game 35 Score 0 Record: 1\n",
      "Game 36 Score 0 Record: 1\n",
      "Game 37 Score 0 Record: 1\n",
      "Game 38 Score 1 Record: 1\n",
      "Game 39 Score 0 Record: 1\n",
      "Game 40 Score 0 Record: 1\n",
      "Game 41 Score 1 Record: 1\n",
      "Game 42 Score 0 Record: 1\n",
      "Game 43 Score 0 Record: 1\n",
      "Game 44 Score 0 Record: 1\n",
      "Game 45 Score 0 Record: 1\n",
      "Game 46 Score 0 Record: 1\n",
      "Game 47 Score 1 Record: 1\n",
      "Game 48 Score 0 Record: 1\n",
      "Game 49 Score 0 Record: 1\n",
      "Game 50 Score 0 Record: 1\n",
      "Game 51 Score 0 Record: 1\n",
      "Game 52 Score 1 Record: 1\n",
      "Game 53 Score 0 Record: 1\n",
      "Game 54 Score 0 Record: 1\n",
      "Game 55 Score 0 Record: 1\n",
      "Game 56 Score 0 Record: 1\n",
      "Game 57 Score 0 Record: 1\n",
      "Game 58 Score 0 Record: 1\n",
      "Game 59 Score 0 Record: 1\n",
      "Game 60 Score 0 Record: 1\n",
      "Game 61 Score 0 Record: 1\n",
      "Game 62 Score 0 Record: 1\n",
      "Game 63 Score 1 Record: 1\n",
      "Game 64 Score 0 Record: 1\n",
      "Game 65 Score 0 Record: 1\n",
      "Game 66 Score 0 Record: 1\n",
      "Game 67 Score 0 Record: 1\n",
      "Game 68 Score 0 Record: 1\n",
      "Game 69 Score 0 Record: 1\n",
      "Game 70 Score 0 Record: 1\n",
      "Game 71 Score 0 Record: 1\n",
      "Game 72 Score 0 Record: 1\n",
      "Game 73 Score 0 Record: 1\n",
      "Game 74 Score 0 Record: 1\n",
      "Game 75 Score 0 Record: 1\n",
      "Game 76 Score 0 Record: 1\n",
      "Game 77 Score 0 Record: 1\n",
      "Game 78 Score 0 Record: 1\n",
      "Game 79 Score 0 Record: 1\n",
      "Game 80 Score 0 Record: 1\n",
      "Game 81 Score 0 Record: 1\n",
      "Game 82 Score 1 Record: 1\n",
      "Game 83 Score 0 Record: 1\n",
      "Game 84 Score 0 Record: 1\n",
      "Game 85 Score 0 Record: 1\n",
      "Game 86 Score 0 Record: 1\n",
      "Game 87 Score 0 Record: 1\n",
      "Game 88 Score 0 Record: 1\n",
      "Game 89 Score 0 Record: 1\n",
      "Game 90 Score 0 Record: 1\n",
      "Game 91 Score 0 Record: 1\n",
      "Game 92 Score 0 Record: 1\n",
      "Game 93 Score 0 Record: 1\n",
      "Game 94 Score 0 Record: 1\n",
      "Game 95 Score 0 Record: 1\n",
      "Game 96 Score 0 Record: 1\n",
      "Game 97 Score 0 Record: 1\n",
      "Game 98 Score 0 Record: 1\n",
      "Game 99 Score 0 Record: 1\n",
      "Game 100 Score 0 Record: 1\n",
      "Game 101 Score 0 Record: 1\n",
      "Game 102 Score 1 Record: 1\n",
      "Game 103 Score 0 Record: 1\n",
      "Game 104 Score 0 Record: 1\n",
      "Game 105 Score 0 Record: 1\n",
      "Game 106 Score 0 Record: 1\n",
      "Game 107 Score 0 Record: 1\n",
      "Game 108 Score 0 Record: 1\n",
      "Game 109 Score 0 Record: 1\n",
      "Game 110 Score 0 Record: 1\n",
      "Game 111 Score 0 Record: 1\n",
      "Game 112 Score 0 Record: 1\n",
      "Game 113 Score 0 Record: 1\n",
      "Game 114 Score 0 Record: 1\n",
      "Game 115 Score 0 Record: 1\n",
      "Game 116 Score 0 Record: 1\n",
      "Game 117 Score 0 Record: 1\n",
      "Game 118 Score 0 Record: 1\n",
      "Game 119 Score 0 Record: 1\n",
      "Game 120 Score 0 Record: 1\n",
      "Game 121 Score 0 Record: 1\n",
      "Game 122 Score 0 Record: 1\n",
      "Game 123 Score 0 Record: 1\n",
      "Game 124 Score 0 Record: 1\n",
      "Game 125 Score 0 Record: 1\n",
      "Game 126 Score 0 Record: 1\n",
      "Game 127 Score 0 Record: 1\n",
      "Game 128 Score 0 Record: 1\n",
      "Game 129 Score 0 Record: 1\n",
      "Game 130 Score 0 Record: 1\n",
      "Game 131 Score 0 Record: 1\n",
      "Game 132 Score 0 Record: 1\n",
      "Game 133 Score 0 Record: 1\n",
      "Game 134 Score 0 Record: 1\n",
      "Game 135 Score 0 Record: 1\n",
      "Game 136 Score 0 Record: 1\n",
      "Game 137 Score 0 Record: 1\n",
      "Game 138 Score 0 Record: 1\n",
      "Game 139 Score 0 Record: 1\n",
      "Game 140 Score 0 Record: 1\n",
      "Game 141 Score 0 Record: 1\n",
      "Game 142 Score 0 Record: 1\n",
      "Game 143 Score 0 Record: 1\n",
      "Game 144 Score 0 Record: 1\n",
      "Game 145 Score 0 Record: 1\n",
      "Game 146 Score 0 Record: 1\n",
      "Game 147 Score 0 Record: 1\n",
      "Game 148 Score 0 Record: 1\n",
      "Game 149 Score 0 Record: 1\n",
      "Game 150 Score 0 Record: 1\n",
      "Game 151 Score 0 Record: 1\n",
      "Game 152 Score 0 Record: 1\n",
      "Game 153 Score 0 Record: 1\n",
      "Game 154 Score 0 Record: 1\n",
      "Game 155 Score 0 Record: 1\n",
      "Game 156 Score 0 Record: 1\n",
      "Game 157 Score 0 Record: 1\n",
      "Game 158 Score 0 Record: 1\n",
      "Game 159 Score 0 Record: 1\n",
      "Game 160 Score 0 Record: 1\n",
      "Game 161 Score 0 Record: 1\n",
      "Game 162 Score 0 Record: 1\n",
      "Game 163 Score 0 Record: 1\n",
      "Game 164 Score 0 Record: 1\n",
      "Game 165 Score 0 Record: 1\n",
      "Game 166 Score 0 Record: 1\n",
      "Game 167 Score 0 Record: 1\n",
      "Game 168 Score 0 Record: 1\n",
      "Game 169 Score 0 Record: 1\n",
      "Game 170 Score 0 Record: 1\n",
      "Game 171 Score 0 Record: 1\n",
      "Game 172 Score 0 Record: 1\n",
      "Game 173 Score 0 Record: 1\n",
      "Game 174 Score 0 Record: 1\n",
      "Game 175 Score 0 Record: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train()\n",
      "\u001b[1;32m/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m game\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m agent\u001b[39m.\u001b[39mn_games \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain_long_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m record:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     record \u001b[39m=\u001b[39m score\n",
      "\u001b[1;32m/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     mini_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmini_sample)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenweichu/dev/study/snake-ai-pytorch/the_game.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain_step(states, actions, rewards, next_states, dones)\n",
      "File \u001b[0;32m~/dev/study/snake-ai-pytorch/model.py:70\u001b[0m, in \u001b[0;36mQTrainer.train_step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     67\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(target, pred)\n\u001b[1;32m     68\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/dev/miniconda3/envs/rl_pygame/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/miniconda3/envs/rl_pygame/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/dev/miniconda3/envs/rl_pygame/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/dev/miniconda3/envs/rl_pygame/lib/python3.8/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/dev/miniconda3/envs/rl_pygame/lib/python3.8/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 393\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_scores = []\n",
    "plot_mean_scores = []\n",
    "total_score = 0\n",
    "record = 0\n",
    "game = SnakeGame(16, 9)\n",
    "agent = Agent(game)\n",
    "while True:\n",
    "    # get old state\n",
    "    state_old = agent.get_state(game)\n",
    "\n",
    "    # get move\n",
    "    final_move = agent.get_action(state_old)\n",
    "\n",
    "    # perform move and get new state\n",
    "    reward, done, score = game.play_step(final_move)\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    # game.render()\n",
    "    # time.sleep(0.01)\n",
    "\n",
    "    state_new = agent.get_state(game)\n",
    "\n",
    "    # train short memory\n",
    "    agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "    # remember\n",
    "    agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "    if done:\n",
    "        # train long memory, plot result\n",
    "        game.reset()\n",
    "        agent.n_games += 1\n",
    "        agent.train_long_memory()\n",
    "\n",
    "        if score > record:\n",
    "            record = score\n",
    "            agent.model.save()\n",
    "\n",
    "        print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "        plot_scores.append(score)\n",
    "        total_score += score\n",
    "        # mean_score = total_score / agent.n_games\n",
    "        mean_score = average_of_last_n_items(plot_scores, 20)\n",
    "        plot_mean_scores.append(mean_score)\n",
    "        # plot(plot_scores, plot_mean_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
